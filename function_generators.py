# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gNmldu441VpiUoRep65fLzsdPdwejUiG
"""

from sklearn.pipeline import Pipeline
import os
# import kaggle
import requests
import urllib.request
import wget
import pandas as pd
import re

import nltk
nltk.download('stopwords')
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np 
import pandas as pd

from termcolor import colored

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
stop_words = stopwords.words('english')
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()

tf.random.set_seed(1234)

import string
table = str.maketrans('', '', string.punctuation)

from sklearn.preprocessing import FunctionTransformer
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

if not os.path.exists("data/github_reddit/train.csv"):
    os.mkdir("data/github_reddit")
    url = 'https://raw.githubusercontent.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech/master/data/reddit.csv'
    wget.download(url,"data/github_reddit/train.csv")


if not os.path.exists("data/github_twitter/train.csv"):
    os.mkdir("data/github_twitter")
    url = 'https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv'
    wget.download(url,"data/github_twitter/train.csv")

"""## Cleaning Sourced Data"""

gt_df = pd.read_csv("data/github_twitter/train.csv")
gr_df = pd.read_csv("data/github_reddit/train.csv")

gr_df['hate_speech_idx'] = gr_df['hate_speech_idx'].fillna(0)
data = list()
label = list()
for index, row in gr_df.iterrows():
    if row["hate_speech_idx"] != 0:
        ini_list = row["hate_speech_idx"]
        res = ini_list.strip('][').split(', ')
        res = [int(item) - 1 for item in res]
        temp = row["text"].replace("\t","").split("\n")
        main = [x[x.find('.') + 2:] for x in temp]
        main = main[:-1]
        main = [x.strip("'") for x in main]
        main = [x.strip('"') for x in main]
        main = [x.lstrip('>') for x in main]
        res = [x for x in res if x < len(main)]
        data = data + [main[x] for x in res]
        label = label + [1 for x in res]
        notres = [x for x in range(len(main)) if x not in res]
        data = data + [main[x] for x in notres]
        label = label + [0 for x in notres]
        
dictionary = {'Label':label, 'Data':data}
train_data1 = pd.DataFrame(dictionary)
if not os.path.exists("data/clean"):
    os.system("mkdir data/clean")
train_data1.to_csv("data/clean/reddit.csv")

clean_tweets = []
clean_labels = []
for index, row in gt_df.iterrows():
  s = re.sub(r'[^a-zA-Z0-9_!@#$%^&*\(\)-= \{\}\[\]:;\"]', '', row['tweet'])
  s = re.sub(r'!+ RT', '', s)
  s = re.sub(r'@.*:','TWUser', s)
  s = re.sub(r'@[a-zA-Z0-9_]*','TWUser', s)
  s = s.strip(" ")
  s = s.strip("\t")
  clean_tweets.append(s)
  if(row['class']==0):
    clean_labels.append(1)
  else:
    clean_labels.append(0)
dictionary = {'Label':clean_labels, 'Data':clean_tweets}
train_data2 = pd.DataFrame(dictionary)

combined = pd.concat([train_data1, train_data2], ignore_index=True)
if not os.path.exists("data/clean"):
    os.system("mkdir data/clean")
combined.to_csv("data/clean/combined.csv")

combined_df = pd.read_csv("data/clean/combined.csv", index_col = "Unnamed: 0")
num_to_sample = np.sum(combined_df['Label']==1)
df_zero = combined_df.query("Label==0").sample(n = num_to_sample, random_state=1)
df_one = combined_df.query("Label==1")
combined_df = df_zero.append(df_one, ignore_index=True)
combined_df = combined_df.sample(frac = 1)
vocab_size = 10000
combined_df.head

class ConvertToLower(BaseEstimator, TransformerMixin):
  def __init__(self, column=None):
        self.column = column

  def fit(self, X, y = None):
    return self

  def transform(self, X, y = None):
    X[self.column] = X[self.column].str.lower()
    return X

class RemovePunctuation(BaseEstimator, TransformerMixin):
  def __init__(self, column=None):
        self.column = column

  def fit(self, X, y = None):
    return self

  def transform(self, X, y = None):
    X[self.column] = X[self.column].apply(lambda x: ' '.join([word.translate(table) for word in str(x).split()]))
    return X

class RemoveStopWords(BaseEstimator, TransformerMixin):
  def __init__(self, column=None):
        self.column = column

  def fit(self, X, y = None):
    return self

  def transform(self, X, y = None):
    X[self.column] = X[self.column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    return X

class StemInput(BaseEstimator, TransformerMixin):
  def __init__(self, column=None):
        self.column = column

  def fit(self, X, y = None):
    return self

  def transform(self, X, y = None):
    X[self.column] = X[self.column].apply(lambda x: ' '.join([porter.stem(word) for word in x.split()]))
    return X

class ToNumpy(BaseEstimator, TransformerMixin):
  def __init__(self, estimator_name):
    self.name = estimator_name
  
  def fit(self, X, y = None):
    return self
  
  def transform(self, X, y = None):
    return X.to_numpy()

class TokeniseInput(BaseEstimator, TransformerMixin):
  def __init__(self, column=None):
        self.tokeniser = Tokenizer(num_words = vocab_size, oov_token = "<oov>")
        self.column = column

  def fit(self, X, y = None):
    self.tokeniser.fit_on_texts(X[self.column])
    return self

  def transform(self, X, y = None):
    X['sequences'] = self.tokeniser.texts_to_sequences(X[self.column])
    return X

class PadSequences(BaseEstimator, TransformerMixin):
  def __init__(self, column=None):
        self.column = column

  def fit(self, X, y = None):
    return self

  def transform(self, X, y = None):
    X_train = pad_sequences(X['sequences'],maxlen=120,truncating='post')
    return X_train
